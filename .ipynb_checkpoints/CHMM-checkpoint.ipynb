{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa4664f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de9ba6",
   "metadata": {},
   "source": [
    "# Data Engineering\n",
    "\n",
    "Suppose we are given amino acid sequence s and class label sequence c. Let n be the length of both sequences. \n",
    "For simplicity, we assume there are 20 amino acids. We create a bijection mapping from each amino acid to [1, 20]. We map each amino acid sequence s to a n sized vector $s'\\in R^n$, where the $i^{th}$ position is its corresponding numerical mapping from its amino acid.\n",
    "\n",
    "In a similar fashion, we perform the same process for secondary structures. We assume there are two types of secondary structures alpha helix and beta sheet and optionally no secondary structure. We map each class label sequence c to a n sized vector $c' \\in R^n$, where the $i^{th}$ position is its corresponding numerical mapping from its class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33758080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_to_vec(s):\n",
    "    amino_acids = list(\"AGILPVFWYDERHKSTCMNQ\")\n",
    "    amino_acids.sort()\n",
    "    aa_mapping = {}\n",
    "    for i, aa in enumerate(amino_acids):\n",
    "        aa_mapping[aa] = i + 1\n",
    "    return list(map(lambda aa: aa_mapping[aa], list(s)))\n",
    "        \n",
    "\n",
    "def ss_to_vec(c):\n",
    "    #TODO\n",
    "    return list(c)\n",
    "\n",
    "# shorthand aliases\n",
    "aa = lambda s: aa_to_vec(s)\n",
    "ss = lambda c: ss_to_vec(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59143219",
   "metadata": {},
   "source": [
    "# Model Parameterization\n",
    "\n",
    "For notion, let $\\theta$ := hidden markov model parameters (state transition probabilities, symbol emission probabilities), and let $\\phi$ := class emission parameters.\n",
    "\n",
    "Our objective function attempts to find maximize the conditionally probability of obtaining class label sequence c given amino acid sequence s, hidden markov parameters $\\theta$, class emission parameters $\\phi$.\n",
    "\n",
    "The number of hidden states usually requires some expert insights. Here, we adopt the hidden markov model setup introduced in assignment two - which includes two hidden states A and B. Then, we have a 4 state transisition probabilities $(t_{aa}, t_{ab}, t_{ba}, t_{bb})$, 20 symbol emission probabilties for state A $(e_{a1}, ... , e_{a20})$, 20 symbol emission probabilties for state B $(e_{b1}, ... , e_{b20})$. Then, we have 44 parameters total for $\\theta$.\n",
    "\n",
    "\n",
    "In this setup, we also have 3 class emission probabilities for state A $(\\phi_{a1}, \\phi_{a2} , \\phi_{a3})$, 3 class emission probabilities for state B $(\\phi_{b1}, \\phi_{b2} , \\phi_{b3})$. \n",
    "\n",
    "For simplicity, we initalize these variables to a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8627d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModelParameters:\n",
    "    def __init__(self, theta):\n",
    "        self.update_theta(theta)\n",
    "        \n",
    "        \n",
    "    def update_theta(self, new_theta):\n",
    "        # Update transition probabilities\n",
    "        self.trans_probs = np.array(new_theta[:4]).reshape(2, 2)\n",
    "\n",
    "        # Update emission probabilities for state A and state B\n",
    "        self.emiss_probs = np.zeros((2, 20))  # Assuming 20 emissions for each state\n",
    "        self.emiss_probs[0, :] = new_theta[4:24]\n",
    "        self.emiss_probs[1, :] = new_theta[24:]   \n",
    "        \n",
    "    def theta(self):\n",
    "        return np.concatenate((self.trans_probs.flatten(), self.emiss_probs.flatten()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce450e6",
   "metadata": {},
   "source": [
    "# Gradient Calculations\n",
    "\n",
    "We can convert our objective function into a minimization problem by changing our objective into minimizing the negative log likelihood.\n",
    "\n",
    "$$\\hat{\\theta} = \\arg \\max_{\\theta} P(c | s, \\theta, \\phi) = \\frac{P(c, s | \\theta, \\phi)}{P(s | \\theta)}$$\n",
    "$$\\hat{\\theta} = \\arg \\min_{\\theta} -\\log(\\frac{P(c, s | \\theta, \\phi)}{P(s | \\theta)})$$\n",
    "\n",
    "We can calculate the gradient of this expression into terms that we know from forward-backward algorithms.\n",
    "\n",
    "$$\\frac{dL}{d\\theta_k} = -\\frac {m_k(c, s) - n_k(s)}{\\theta_k}$$\n",
    "\n",
    "$$n_k(s) := idk$$\n",
    "\n",
    "$$m_k(c, s) := idk$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0a7ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel(HiddenMarkovModelParameters):\n",
    "    def __init__(self, theta):\n",
    "        super().__init__(theta)\n",
    "    \n",
    "    \n",
    "    def forward_backward(self, obs, class_labels=None):\n",
    "        num_states = 2\n",
    "        num_observations = len(obs)\n",
    "\n",
    "        forward = np.zeros((num_states, num_observations))\n",
    "        backward = np.zeros((num_states, num_observations))\n",
    "        n_k = np.zeros(4 + 2 * 20)  # Transition + Emission probabilities\n",
    "        m_k = np.zeros_like(n_k) if class_labels is not None else None\n",
    "\n",
    "        # Forward Pass\n",
    "        forward[:, 0] = self.emiss_probs[:, obs[0]] / num_states\n",
    "        for t in range(1, num_observations):\n",
    "            for s in range(num_states):\n",
    "                for prev_s in range(num_states):\n",
    "                    forward[s, t] += forward[prev_s, t-1] * self.trans_probs[prev_s, s] * self.emiss_probs[s, obs[t]]\n",
    "                    n_k_index = prev_s * num_states + s\n",
    "                    n_k[n_k_index] += forward[prev_s, t-1] * self.trans_probs[prev_s, s]\n",
    "                    if class_labels is not None and class_labels[t] == s:\n",
    "                        m_k[n_k_index] += forward[prev_s, t-1] * self.trans_probs[prev_s, s]\n",
    "\n",
    "                n_k[4 + s * 20: 4 + (s + 1) * 20] += self.emiss_probs[s, obs[t]]\n",
    "                if class_labels is not None and class_labels[t] == s:\n",
    "                    m_k[4 + s * 20: 4 + (s + 1) * 20] += self.emiss_probs[s, obs[t]]\n",
    "\n",
    "        # Backward Pass\n",
    "        backward[:, -1] = 1\n",
    "        for t in range(num_observations - 2, -1, -1):\n",
    "            for s in range(num_states):\n",
    "                backward[s, t] = np.sum(backward[:, t+1] * self.trans_probs[s, :] * self.emiss_probs[:, obs[t+1]])\n",
    "\n",
    "        # Normalize n_k and m_k by the total number of observations\n",
    "        n_k /= num_observations\n",
    "        if m_k is not None:\n",
    "            m_k /= num_observations\n",
    "\n",
    "        return forward, backward, n_k, m_k\n",
    "    \n",
    "    \n",
    "    def calculate_gradient(self, mk, nk):\n",
    "        # Ensure mk and nk are provided and have the correct shape\n",
    "        if mk is None or nk is None:\n",
    "            raise ValueError(\"mk and nk must be provided and have the correct shape.\")\n",
    "\n",
    "        # Flatten the transition and emission probabilities for gradient calculation\n",
    "        theta_flat = np.concatenate((self.trans_probs.flatten(), self.emiss_probs.flatten()))\n",
    "\n",
    "        # Calculate the gradient\n",
    "        gradient = -(mk - nk) / theta_flat\n",
    "\n",
    "        return gradient\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d9c6c",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "We use gradient descent to minimize our objective function.\n",
    "\n",
    "We repeat the following operation until convergence. $\\theta'=\\theta - \\alpha \\nabla L$. For simplicity, we fix our step size $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "800f9fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class HiddenMarkovModelWithGradientDescent(HiddenMarkovModel):\n",
    "    def __init__(self, theta):\n",
    "        super().__init__(theta)\n",
    "    \n",
    "    def gradient_descent(self, observations_set, class_labels_set, learning_rate=0.01, iterations=100):\n",
    "        if len(observations_set) != len(class_labels_set):\n",
    "            raise ValueError(\"Each observation sequence must have corresponding class labels.\")\n",
    "\n",
    "        # Initialize the total gradient array\n",
    "        total_params = len(self.trans_probs.flatten()) + len(self.emiss_probs.flatten())\n",
    "        total_gradient = np.zeros(total_params)\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            # Reset the gradient for this iteration\n",
    "            total_gradient.fill(0)\n",
    "\n",
    "            # Accumulate gradients over all observation sequences\n",
    "            for obs, class_labels in zip(observations_set, class_labels_set):\n",
    "                _, _, nk, mk = self.forward_backward(obs, class_labels)\n",
    "                gradient = self.calculate_gradient(mk, nk)\n",
    "                total_gradient += gradient\n",
    "\n",
    "            # Average the gradient across all sequences\n",
    "            avg_gradient = total_gradient / len(observations_set)\n",
    "\n",
    "            # Update parameters\n",
    "            theta_flat = np.concatenate((self.trans_probs.flatten(), self.emiss_probs.flatten()))\n",
    "            theta_flat -= learning_rate * avg_gradient\n",
    "\n",
    "            # Reshape and update the model parameters\n",
    "            self.trans_probs = theta_flat[:4].reshape(2, 2)\n",
    "            self.emiss_probs = theta_flat[4:].reshape(2, 20)\n",
    "\n",
    "        # After iterations, update the theta attribute to the new values\n",
    "        self.update_theta(theta_flat)\n",
    "        \n",
    "theta = [0.7, 0.3, 0.4, 0.6] + [0.05] * 20 + [0.03] * 20\n",
    "hmm = HiddenMarkovModelWithGradientDescent(theta)\n",
    "\n",
    "# Example multiple observation sequences and corresponding class labels\n",
    "observations_set = [[0, 1, 2, 3, 4], [1, 0, 3, 2, 4]]  # Add more sequences as needed\n",
    "class_labels_set = [[1, 0, 1, 0, 1], [0, 1, 0, 1, 0]]  # Corresponding class labels\n",
    "\n",
    "learning_rate = 0.01\n",
    "iterations = 100\n",
    "\n",
    "hmm.gradient_descent(observations_set, class_labels_set, learning_rate, iterations)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648d995",
   "metadata": {},
   "source": [
    "# Validation Results\n",
    "\n",
    "Since our dataset if limited, we approximate our validation error using k-cross-folds validation in particular we use LOOCV (leave-one-out-cross-validation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65fa56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, s):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def error_est(s_data, c_data):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75671fcf",
   "metadata": {},
   "source": [
    "# Workflow on Human Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"HUMAN_training_data.txt\") as f:\n",
    "    content = f.read().split(\"\\n\")\n",
    "    s_seqs = []\n",
    "    c_seqs = []\n",
    "    for i in range(0, len(content), 2):\n",
    "        s_seqs.append(aa(content[i]))\n",
    "    for i in range(1, len(content), 2):\n",
    "        c_seqs.append(ss(content[i]))\n",
    "    \n",
    "    human_train = zip(s_seqs, c_seqs)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
