{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa4664f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de9ba6",
   "metadata": {},
   "source": [
    "# Data Engineering\n",
    "\n",
    "Suppose we are given amino acid sequence s and class label sequence c. Let n be the length of both sequences. \n",
    "For simplicity, we assume there are 20 amino acids. We create a bijection mapping from each amino acid to [1, 20]. We map each amino acid sequence s to a n sized vector $s'\\in R^n$, where the $i^{th}$ position is its corresponding numerical mapping from its amino acid.\n",
    "\n",
    "In a similar fashion, we perform the same process for secondary structures. We assume there are two types of secondary structures alpha helix and beta sheet and optionally no secondary structure. We map each class label sequence c to a n sized vector $c' \\in R^n$, where the $i^{th}$ position is its corresponding numerical mapping from its class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "33758080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_to_vec(s):\n",
    "    amino_acids = list(\"AGILPVFWYDERHKSTCMNQ\")\n",
    "    amino_acids.sort()\n",
    "    aa_mapping = {}\n",
    "    for i, aa in enumerate(amino_acids):\n",
    "        aa_mapping[aa] = i\n",
    "    return list(map(lambda aa: aa_mapping[aa], list(s)))\n",
    "        \n",
    "\n",
    "def ss_to_vec(c):\n",
    "    #TODO\n",
    "    return list(map(lambda lbl: int(lbl) - 1, list(c)))\n",
    "\n",
    "# shorthand aliases\n",
    "aa = lambda s: aa_to_vec(s)\n",
    "ss = lambda c: ss_to_vec(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59143219",
   "metadata": {},
   "source": [
    "# Model Parameterization\n",
    "\n",
    "For notion, let $\\theta$ := hidden markov model parameters (state transition probabilities, symbol emission probabilities), and let $\\phi$ := class emission parameters.\n",
    "\n",
    "Our objective function attempts to find maximize the conditionally probability of obtaining class label sequence c given amino acid sequence s, hidden markov parameters $\\theta$, class emission parameters $\\phi$.\n",
    "\n",
    "The number of hidden states usually requires some expert insights. Here, we adopt the hidden markov model setup introduced in assignment two - which includes two hidden states A and B. Then, we have a 4 state transisition probabilities $(t_{aa}, t_{ab}, t_{ba}, t_{bb})$, 20 symbol emission probabilties for state A $(e_{a1}, ... , e_{a20})$, 20 symbol emission probabilties for state B $(e_{b1}, ... , e_{b20})$. Then, we have 44 parameters total for $\\theta$.\n",
    "\n",
    "\n",
    "In this setup, we also have 3 class emission probabilities for state A $(\\phi_{a1}, \\phi_{a2} , \\phi_{a3})$, 3 class emission probabilities for state B $(\\phi_{b1}, \\phi_{b2} , \\phi_{b3})$. \n",
    "\n",
    "For simplicity, we initalize these variables to a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8627d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModelParameters:\n",
    "    \n",
    "    \n",
    "    def __init__(self, theta, phi=None):\n",
    "        self.update_theta(theta)\n",
    "        self.class_emiss_probs = np.ones((2, 3)) / 3\n",
    "        if phi is None:\n",
    "            self.phi = np.ones((2, 3)) / 3  # Uniform initialization if phi not provided\n",
    "        else:\n",
    "            self.update_phi(phi)\n",
    "        \n",
    "        \n",
    "    def update_theta(self, new_theta):\n",
    "        # Update transition probabilities\n",
    "        self.trans_probs = np.array(new_theta[:4]).reshape(2, 2)\n",
    "\n",
    "        # Update emission probabilities for state A and state B\n",
    "        self.emiss_probs = np.zeros((2, 20))  # Assuming 20 emissions for each state\n",
    "        self.emiss_probs[0, :] = new_theta[4:24]\n",
    "        self.emiss_probs[1, :] = new_theta[24:] \n",
    "    \n",
    "    \n",
    "    def update_phi(self, new_phi):\n",
    "        self.phi = np.array(new_phi).reshape(2, 3)\n",
    "        \n",
    "        \n",
    "    def get_theta(self):\n",
    "        return np.concatenate((self.trans_probs.flatten(), self.emiss_probs.flatten()))\n",
    "    \n",
    "    \n",
    "    def get_phi(self):\n",
    "        return self.phi.flatten()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce450e6",
   "metadata": {},
   "source": [
    "# Gradient Calculations\n",
    "\n",
    "We can convert our objective function into a minimization problem by changing our objective into minimizing the negative log likelihood.\n",
    "\n",
    "$$\\hat{\\theta} = \\arg \\max_{\\theta} P(c | s, \\theta, \\phi) = \\frac{P(c, s | \\theta, \\phi)}{P(s | \\theta)}$$\n",
    "$$\\hat{\\theta} = \\arg \\min_{\\theta} -\\log(\\frac{P(c, s | \\theta, \\phi)}{P(s | \\theta)})$$\n",
    "\n",
    "We can calculate the gradient of this expression into terms that we know from forward-backward algorithms.\n",
    "\n",
    "$$\\frac{dL}{d\\theta_k} = -\\frac {m_k(c, s) - n_k(s)}{\\theta_k}$$\n",
    "\n",
    "$$n_k(s) := idk$$\n",
    "\n",
    "$$m_k(c, s) := idk$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e0a7ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel(HiddenMarkovModelParameters):\n",
    "    \n",
    "    \n",
    "    def __init__(self, theta, phi=None):\n",
    "        super().__init__(theta, phi)\n",
    "    \n",
    "    \n",
    "    def log_sum_exp(self, arr):\n",
    "        # Log-sum-exp trick for numerical stability in log-space\n",
    "        max_val = np.max(arr)\n",
    "        return max_val + np.log(np.sum(np.exp(arr - max_val)))\n",
    "    \n",
    "    \n",
    "    def calculate_theta_gradient(self, mk, nk):\n",
    "        if mk is None or nk is None:\n",
    "            raise ValueError(\"mk and nk must be provided and have the correct shape.\")\n",
    "\n",
    "        theta_flat = np.concatenate((self.trans_probs.flatten(), self.emiss_probs.flatten()))\n",
    "        theta_flat = np.clip(theta_flat, 1e-10, None)\n",
    "        gradient = -(mk - nk) / theta_flat\n",
    "\n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "    def calculate_phi_gradient(self, q_k):\n",
    "        # Calculate gradient for phi using -q_k(c, s) / phi_k\n",
    "        # Ensure no division by zero\n",
    "        phi_k_nonzero = np.clip(self.phi, 1e-10, None)\n",
    "        phi_gradient = -q_k / phi_k_nonzero\n",
    "        return phi_gradient\n",
    "    \n",
    "    \n",
    "    def forward_backward(self, obs, class_labels=None):\n",
    "        num_states = 2\n",
    "        num_observations = len(obs)\n",
    "\n",
    "        forward = np.zeros((num_states, num_observations))\n",
    "        backward = np.zeros((num_states, num_observations))\n",
    "        n_k = np.zeros(4 + 2 * 20)  # Transition + Emission probabilities\n",
    "        m_k = np.zeros_like(n_k)    # Expected usage of model parameters given class labels\n",
    "        q_k = np.zeros((2, 3))      # Expected usage of class emission parameters\n",
    "\n",
    "        # Forward Pass\n",
    "        forward[:, 0] = self.emiss_probs[:, obs[0]] / num_states\n",
    "        for t in range(1, num_observations):\n",
    "            for s in range(num_states):\n",
    "                forward[s, t] = 0\n",
    "                for prev_s in range(num_states):\n",
    "                    forward_prob = forward[prev_s, t-1] * self.trans_probs[prev_s, s] * self.emiss_probs[s, obs[t]]\n",
    "                    forward[s, t] += forward_prob\n",
    "\n",
    "                    # Update n_k for transition and emission probabilities\n",
    "                    n_k_index = prev_s * num_states + s\n",
    "                    n_k[n_k_index] += forward_prob\n",
    "                    n_k[4 + s * 20 + obs[t]] += forward_prob\n",
    "\n",
    "                    # Update m_k if class_labels are provided\n",
    "                    if class_labels is not None and class_labels[t] == s:\n",
    "                        m_k[n_k_index] += forward_prob\n",
    "                        m_k[4 + s * 20 + obs[t]] += forward_prob\n",
    "\n",
    "                # Update q_k for class emission probabilities\n",
    "                if class_labels is not None:\n",
    "                    q_k[s, class_labels[t]] += forward[s, t]\n",
    "\n",
    "        # Backward Pass\n",
    "        backward[:, -1] = 1\n",
    "        for t in range(num_observations - 2, -1, -1):\n",
    "            for s in range(num_states):\n",
    "                backward[s, t] = sum(backward[next_s, t+1] * self.trans_probs[s, next_s] * self.emiss_probs[next_s, obs[t+1]] for next_s in range(num_states))\n",
    "\n",
    "        # Normalize n_k, m_k, and q_k by the total number of observations\n",
    "        n_k /= num_observations\n",
    "        m_k /= num_observations\n",
    "        q_k /= num_observations\n",
    "\n",
    "        # Update class emission probabilities\n",
    "        for state in range(num_states):\n",
    "            self.phi[state] = q_k[state] / q_k[state].sum()\n",
    "\n",
    "        return forward, backward, n_k, m_k, q_k\n",
    "    \n",
    "    \n",
    "    def predict_class_sequence(self, obs_sequence):\n",
    "        num_states = self.trans_probs.shape[0]\n",
    "        num_observations = len(obs_sequence)\n",
    "\n",
    "        # Initialize Viterbi matrices\n",
    "        viterbi = np.zeros((num_states, num_observations))\n",
    "        backpointer = np.zeros((num_states, num_observations), dtype=int)\n",
    "\n",
    "        # Initialization step\n",
    "        viterbi[:, 0] = self.emiss_probs[:, obs_sequence[0]] * self.phi[:, 0]\n",
    "\n",
    "        # Viterbi algorithm - Dynamic Programming\n",
    "        for t in range(1, num_observations):\n",
    "            for s in range(num_states):\n",
    "                transition_probs = viterbi[:, t-1] * self.trans_probs[:, s]\n",
    "                max_prob = np.max(transition_probs)\n",
    "                backpointer[s, t] = np.argmax(transition_probs)\n",
    "\n",
    "                # Incorporate emission probabilities and class probabilities\n",
    "                viterbi[s, t] = max_prob * self.emiss_probs[s, obs_sequence[t]] * self.phi[s, 0]\n",
    "\n",
    "        # Backtracking to find the most probable path and class labels\n",
    "        most_probable_path = np.zeros(num_observations, dtype=int)\n",
    "        most_probable_classes = np.zeros(num_observations, dtype=int)\n",
    "        most_probable_path[-1] = np.argmax(viterbi[:, -1])\n",
    "\n",
    "        for t in range(num_observations - 2, -1, -1):\n",
    "            most_probable_path[t] = backpointer[most_probable_path[t+1], t+1]\n",
    "            most_probable_classes[t] = np.argmax(self.phi[most_probable_path[t], :])\n",
    "\n",
    "        return most_probable_path, most_probable_classes\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, observations_set, class_labels_set, learning_rate=0.01, iterations=100, convergence_threshold=1e-6):\n",
    "        prev_theta = self.get_theta()\n",
    "        prev_phi = self.phi.copy()\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            total_theta_gradient = np.zeros_like(prev_theta)\n",
    "            total_phi_gradient = np.zeros_like(prev_phi)\n",
    "\n",
    "            for obs, class_labels in zip(observations_set, class_labels_set):\n",
    "                # Ensure the class label sequence matches the observation sequence in length\n",
    "                if len(obs) != len(class_labels):\n",
    "                    raise ValueError(\"Length of observation sequence and class label sequence must match.\")\n",
    "\n",
    "                _, _, nk, mk, q_k = self.forward_backward(obs, class_labels)\n",
    "                theta_gradient = self.calculate_theta_gradient(mk, nk)\n",
    "                phi_gradient = self.calculate_phi_gradient(q_k)\n",
    "\n",
    "                total_theta_gradient += theta_gradient\n",
    "                total_phi_gradient += phi_gradient\n",
    "\n",
    "            # Average the gradients across all sequences\n",
    "            avg_theta_gradient = total_theta_gradient / len(observations_set)\n",
    "            avg_phi_gradient = total_phi_gradient / len(observations_set)\n",
    "\n",
    "            # Update theta and phi in the direction of the negative average gradients\n",
    "            updated_theta = prev_theta - learning_rate * avg_theta_gradient\n",
    "            updated_phi = prev_phi - learning_rate * avg_phi_gradient\n",
    "\n",
    "            # Constrain updated parameters\n",
    "            updated_theta[:4] = np.clip(updated_theta[:4], 0, 1)\n",
    "            updated_phi = np.clip(updated_phi, 0, 1)\n",
    "\n",
    "            if np.linalg.norm(updated_theta - prev_theta) < convergence_threshold and \\\n",
    "               np.linalg.norm(updated_phi - prev_phi) < convergence_threshold:\n",
    "                break\n",
    "\n",
    "            prev_theta = updated_theta\n",
    "            prev_phi = updated_phi\n",
    "\n",
    "        self.update_theta(updated_theta)\n",
    "        self.update_phi(updated_phi)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d9c6c",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "We use gradient descent to minimize our objective function.\n",
    "\n",
    "We repeat the following operation until convergence. $\\theta'=\\theta - \\alpha \\nabla L$. For simplicity, we fix our step size $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "800f9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModelWithGradientDescent(HiddenMarkovModel):\n",
    "    \n",
    "    \n",
    "    def __init__(self, theta, phi=None):\n",
    "        super().__init__(theta, phi)\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, observations_set, class_labels_set, learning_rate=0.01, iterations=100, convergence_threshold=1e-6):\n",
    "        prev_theta = self.get_theta()\n",
    "        prev_phi = self.phi.copy()\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            total_theta_gradient = np.zeros_like(prev_theta)\n",
    "            total_phi_gradient = np.zeros_like(prev_phi)\n",
    "\n",
    "            for obs, class_labels in zip(observations_set, class_labels_set):\n",
    "                # Ensure the class label sequence matches the observation sequence in length\n",
    "                if len(obs) != len(class_labels):\n",
    "                    raise ValueError(\"Length of observation sequence and class label sequence must match.\")\n",
    "\n",
    "                _, _, nk, mk, q_k = self.forward_backward(obs, class_labels)\n",
    "                theta_gradient = self.calculate_theta_gradient(mk, nk)\n",
    "                phi_gradient = self.calculate_phi_gradient(q_k)\n",
    "\n",
    "                total_theta_gradient += theta_gradient\n",
    "                total_phi_gradient += phi_gradient\n",
    "\n",
    "            # Average the gradients across all sequences\n",
    "            avg_theta_gradient = total_theta_gradient / len(observations_set)\n",
    "            avg_phi_gradient = total_phi_gradient / len(observations_set)\n",
    "\n",
    "            # Update theta and phi in the direction of the negative average gradients\n",
    "            updated_theta = prev_theta - learning_rate * avg_theta_gradient\n",
    "            updated_phi = prev_phi - learning_rate * avg_phi_gradient\n",
    "\n",
    "            # Constrain updated parameters\n",
    "            updated_theta[:4] = np.clip(updated_theta[:4], 0, 1)\n",
    "            updated_phi = np.clip(updated_phi, 0, 1)\n",
    "\n",
    "            if np.linalg.norm(updated_theta - prev_theta) < convergence_threshold and \\\n",
    "               np.linalg.norm(updated_phi - prev_phi) < convergence_threshold:\n",
    "                break\n",
    "\n",
    "            prev_theta = updated_theta\n",
    "            prev_phi = updated_phi\n",
    "\n",
    "        self.update_theta(updated_theta)\n",
    "        self.update_phi(updated_phi)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648d995",
   "metadata": {},
   "source": [
    "# Workflow on Human Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "65fa56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable state sequence: [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "Most probable class sequence: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 0]\n"
     ]
    }
   ],
   "source": [
    "with open(\"HUMAN_training_data.txt\") as f:\n",
    "    content = f.read().split(\"\\n\")\n",
    "    observations_set = []\n",
    "    class_labels_set = []\n",
    "    for i in range(0, len(content), 2):\n",
    "        observations_set.append(aa(content[i]))\n",
    "    for i in range(1, len(content), 2):\n",
    "        class_labels_set.append(ss(content[i]))\n",
    "    \n",
    "    # Initialize transition probabilities uniformly\n",
    "    transition_probs = [0.5, 0.5, 0.5, 0.5]  # 2 states, so 2x2 transition matrix, each entry is 0.5\n",
    "\n",
    "    # Initialize emission probabilities uniformly\n",
    "    emission_probs = [1.0 / 20] * 40  # 20 emissions for each of the 2 states, each probability is 1/20\n",
    "\n",
    "    # Combine to form initial theta\n",
    "    theta_initial = transition_probs + emission_probs\n",
    "\n",
    "    # Initialize class emission probabilities uniformly\n",
    "    phi_initial = [[1.0 / 3, 1.0 / 3, 1.0 / 3],  # State 1\n",
    "                   [1.0 / 3, 1.0 / 3, 1.0 / 3]]  # State 2\n",
    "\n",
    "    \n",
    "    # Initialize the model\n",
    "    hmm = HiddenMarkovModelWithGradientDescent(theta_initial, phi_initial)\n",
    "\n",
    "    \n",
    "    # Set learning rate, number of iterations, and convergence threshold\n",
    "    learning_rate = 0.01\n",
    "    iterations = 100\n",
    "    convergence_threshold = 1e-4\n",
    "\n",
    "    # Run gradient descent\n",
    "    hmm.gradient_descent(observations_set, class_labels_set, learning_rate, iterations, convergence_threshold)\n",
    "\n",
    "    \n",
    "    obs_sequence = aa(\"MENFQKVEKIGEGTYGVVYKARNKLTGEVVALKKIRLDTETEGVPSTAIREISLLKELNHPNIVKLLDVIHTENKLYLVFEFLHQDLKKFMDASALTGIPLPLIKSYLFQLLQGLAFCHSHRVLHRDLKPQNLLINTEGAIKLADFGLARAFGVPVRTYTHEVVTLWYRAPEILLGCKYYSTAVDIWSLGCIFAEMVTRRALFPGDSEIDQLFRIFRTLGTPDEVVWPGVTSMPDYKPSFPKWARQDFSKVVPPLDEDGRSLLSQMLHYDPNKRISAKAALAHPFFQDVTKPVPHLRL\")  # Example sequence\n",
    "    state_sequence, class_sequence = hmm.predict_class_sequence(obs_sequence)\n",
    "    print(\"Most probable class sequence:\", class_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff5776",
   "metadata": {},
   "source": [
    "# Validation Results\n",
    "\n",
    "Since our dataset if limited, we approximate our validation error using k-cross-folds validation in particular we use LOOCV (leave-one-out-cross-validation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"HUMAN_training_data.txt\") as f:\n",
    "    content = f.read().split(\"\\n\")\n",
    "    s_seqs = []\n",
    "    c_seqs = []\n",
    "    for i in range(0, len(content), 2):\n",
    "        s_seqs.append(aa(content[i]))\n",
    "    for i in range(1, len(content), 2):\n",
    "        c_seqs.append(ss(content[i]))\n",
    "    \n",
    "    human_train = zip(s_seqs, c_seqs)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
