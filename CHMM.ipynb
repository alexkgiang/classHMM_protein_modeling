{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa4664f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9ee77",
   "metadata": {},
   "source": [
    "# Data Engineering\n",
    "\n",
    "Suppose we are given amino acid sequence s and class label sequence c. Let n be the length of both sequences. \n",
    "For simplicity, we assume there are 20 amino acids. We create a bijection mapping from each amino acid to [1, 20]. We map each amino acid sequence s to a n sized vector $s'\\in R^n$, where the $i^{th}$ position is its corresponding numerical mapping from its amino acid.\n",
    "\n",
    "In a similar fashion, we perform the same process for secondary structures. We assume there are two types of secondary structures alpha helix and beta sheet and optionally no secondary structure. We map each class label sequence c to a n sized vector $c' \\in R^n$, where the $i^{th}$ position is its corresponding numerical mapping from its class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33758080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_to_vec(s):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def ss_to_vec(c):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43011c2",
   "metadata": {},
   "source": [
    "# Model Parameterization\n",
    "\n",
    "For notion, let $\\theta$ := hidden markov model parameters (state transition probabilities, symbol emission probabilities), and let $\\phi$ := class emission parameters.\n",
    "\n",
    "Our objective function attempts to find maximize the conditionally probability of obtaining class label sequence c given amino acid sequence s, hidden markov parameters $\\theta$, class emission parameters $\\phi$.\n",
    "\n",
    "The number of hidden states usually requires some expert insights. Here, we adopt the hidden markov model setup introduced in assignment two - which includes two hidden states A and B. Then, we have a 4 state transisition probabilities $(t_{aa}, t_{ab}, t_{ba}, t_{bb})$, 20 symbol emission probabilties for state A $(e_{a1}, ... , e_{a20})$, 20 symbol emission probabilties for state B $(e_{b1}, ... , e_{b20})$. Then, we have 44 parameters total for $\\theta$.\n",
    "\n",
    "\n",
    "In this setup, we also have 3 class emission probabilities for state A $(\\phi_{a1}, \\phi_{a2} , \\phi_{a3})$, 3 class emission probabilities for state B $(\\phi_{b1}, \\phi_{b2} , \\phi_{b3})$. \n",
    "\n",
    "For simplicity, we initalize these variables to a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8627d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta():\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def init_phi():\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29381256",
   "metadata": {},
   "source": [
    "# Gradient Calculations\n",
    "\n",
    "We can convert our objective function into a minimization problem by changing our objective into minimizing the negative log likelihood.\n",
    "\n",
    "$$\\hat{\\theta} = \\arg \\max_{\\theta} P(c | s, \\theta, \\phi) = \\frac{P(c, s | \\theta, \\phi)}{P(s | \\theta)}$$\n",
    "$$\\hat{\\theta} = \\arg \\min_{\\theta} -\\log(\\frac{P(c, s | \\theta, \\phi)}{P(s | \\theta)})$$\n",
    "\n",
    "We can calculate the gradient of this expression into terms that we know from forward-backward algorithms.\n",
    "\n",
    "$$\\frac{dL}{d\\theta_k} = -\\frac {m_k(c, s) - n_k(s)}{\\theta_k}$$\n",
    "\n",
    "$$n_k(s) := idk$$\n",
    "\n",
    "$$m_k(c, s) := idk$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a7ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_k(s):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def m_k(c, s):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def gradient(c, s, theta):\n",
    "    #TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c017b",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "We use gradient descent to minimize our objective function.\n",
    "\n",
    "We repeat the following operation until convergence. $\\theta'=\\theta - \\alpha \\nabla L$. For simplicity, we fix our step size $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_gd(init_theta, step_size):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54121286",
   "metadata": {},
   "source": [
    "# Validation Results\n",
    "\n",
    "Since our dataset if limited, we approximate our validation error using k-cross-folds validation in particular we use LOOCV (leave-one-out-cross-validation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65fa56d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3756353947.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def error_est(theta):\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def predict(theta, s):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def error_est(training):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41e8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d3c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
